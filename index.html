<!DOCTYPE HTML>
<html lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

  <title>Albert Wilcox</title>
  
  <meta name="author" content="Albert Wilcox">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  
  <link rel="stylesheet" type="text/css" href="stylesheet.css">
  <link rel="icon" type="image/png" href="images/seal_icon.png">
</head>

<base target="_blank">

<body>
  <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
    <tr style="padding:0px">
      <td style="padding:0px">
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr style="padding:0px">
            <td style="padding:2.5%;width:63%;vertical-align:middle">
              <p style="text-align:center">
                <name>Albert Wilcox</name>
              </p>
              <p>I am an undergraduate in the <a href="https://berkeley.edu">UC Berkeley</a> <a href="https://ls.berkeley.edu/home">
                College of Letters and Sciences</a> studying computer science and applied math.
                I work as a researcher in <a href="http://autolab.berkeley.edu/">UC Berkeley AUTOLab</a>
                advised by Professor <a href="https://goldberg.berkeley.edu/">Ken Goldberg</a>
                where I study robotic automation.
              </p>

              <p>
                Beyond research, I enjoy outdoor activities of any sort, playing guitar, and endurance
                sports (I'm on Berkeley's <a href="https://www.caltriathlon.com/">triathlon team</a>).
                Feel free to follow me on <a href="https://www.strava.com/athletes/21827840">Strava</a>!
              </p>
<!--              <p>-->
<!--                At Google I've worked on <a href="https://ai.googleblog.com/2020/12/portrait-light-enhancing-portrait.html">Portrait Light</a>, <a href="https://ai.googleblog.com/2014/04/lens-blur-in-new-google-camera-app.html">Lens Blur</a>, <a href="https://ai.googleblog.com/2014/10/hdr-low-light-and-high-dynamic-range.html">HDR+</a>, <a href="https://www.google.com/get/cardboard/jump/">Jump</a>, <a href="https://ai.googleblog.com/2017/10/portrait-mode-on-pixel-2-and-pixel-2-xl.html">Portrait Mode</a>, and <a href="https://www.youtube.com/watch?v=JSnB06um5r4">Glass</a>. I did my PhD at <a href="http://www.eecs.berkeley.edu/">UC Berkeley</a>, where I was advised by <a href="http://www.cs.berkeley.edu/~malik/">Jitendra Malik</a> and funded by the <a href="http://www.nsfgrfp.org/">NSF GRFP</a>. I did my bachelors at the <a href="http://cs.toronto.edu">University of Toronto</a>.-->
<!--                I've received the <a href="https://www2.eecs.berkeley.edu/Students/Awards/15/">C.V. Ramamoorthy Distinguished Research Award</a> and the <a href="https://www.thecvf.com/?page_id=413#YRA">PAMI Young Researcher Award</a>.-->
<!--              </p>-->
              <p style="text-align:center">
                <a href="mailto:albertwilcox@berkeley.edu">Email</a> &nbsp/&nbsp
                <a href="data/albert-wilcox-cv.pdf">CV</a> &nbsp/&nbsp
<!--                <a href="data/JonBarron-bio.txt">Bio</a> &nbsp/&nbsp-->
                <a href="https://scholar.google.com/citations?user=bj628LsAAAAJ&hl=en">Google Scholar</a> &nbsp/&nbsp
                <a href="https://twitter.com/albertwilcoxiii">Twitter</a> &nbsp/&nbsp
                <a href="https://github.com/albertwilcox">Github</a>
              </p>
            </td>
            <td style="padding:2.5%;width:40%;max-width:40%">
              <a href="images/albert.jpg"><img style="width:100%;max-width:100%" alt="profile photo" src="images/albert.jpg" class="hoverZoomLink"></a>
            </td>
          </tr>
        </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <heading>Research</heading>
              <p>
                I'm broadly interested in learning for decision making and control, and have spent time working with
                reinforcement learning, imitation learning, representation learning and computer vision.
                Much of my research has been focused on applications to robotic automation.
              </p>
            </td>
          </tr>
        </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>

          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img style="width:105%;max-width:105%" src="images/ls3.png">
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://drive.google.com/file/d/18kd9IvrNLeko6vYRPr2YFZMfOZGRM44g/view">
                <papertitle>Monte Carlo Augmented Actor-Critic for Sparse Reward Deep Reinforcement Learning from Suboptimal Demonstrations</papertitle>
              </a>
              <br>
              <b>Albert Wilcox</b>, Zaynah Javed, Ashwin Balakrishna, Daniel Brown, Ken Goldberg
              <br>
              <em style="font-style:normal"><i>Under Review, 2022.</i></em>
              <br>
              <a href="https://drive.google.com/file/d/18kd9IvrNLeko6vYRPr2YFZMfOZGRM44g/view">PDF</a>
              <br>
              <p></p>
              <p>An easy-to-implement change that can be made to any off-policy actor critic algorithm to speed up and stabilize sparse reward deep reinforcement learning from demonstrations.</p>
            </td>
          </tr>

          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img style="width:105%;max-width:105%" src="images/houston.png">
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://sites.google.com/berkeley.edu/autolab-houston">
                <papertitle>Learning to Localize, Grasp and Hand Over Unmodified Surgical Needles</papertitle>
              </a>
              <br>
              <b>Albert Wilcox</b>, Justin Kerr*, Brijen Thananjeyan, Jeff Ichnowski, Minho Hwang, Samuel Paradis, Danyal Fer, Ken Goldberg
              <br>
              <em style="font-style:normal"><i>To appear at the IEEE International Conference on Robotics and Automation, 2022.</i></em>
              <br>
              <a href="https://arxiv.org/abs/2112.04071">PDF</a> /
              <a href="https://sites.google.com/berkeley.edu/autolab-houston">Website</a>
              <br>
              <p></p>
              <p>An algorithm combining active sensing, perception and imitation learning to reliably hand unmodified surgical needles from one arm to the other on a daVinci Research Kit surgical robot.</p>
            </td>
          </tr>

          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img style="width:105%;max-width:105%" src="images/ls3.png">
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://openreview.net/forum?id=kgoWLlA33-U">
                <papertitle>LS3: Latent Space Safe Sets for Long-Horizon Visuomotor Control of Iterative Tasks</papertitle>
              </a>
              <br>
              <b>Albert Wilcox*</b>, Ashwin Balakrishna*, Brijen Thananjeyan, Joseph E. Gonzalez, Ken Goldberg
              <br>
              <em style="font-style:normal"><i>Conference on Robot Learning (CoRL) 2021.</i></em>
              <br>
              <a href="https://arxiv.org/pdf/2107.04775.pdf">PDF</a> /
              <a href="https://sites.google.com/view/latentspacesafesets">Website</a> /
              <a href="data/ls3-bib.txt">Bibtex</a>
              <br>
              <p></p>
              <p>Safe and efficient RL from image observations by leveraging suboptimal demonstrations to structure exploration and examples of constraint violations to satisfy user-specified constraints.</p>
            </td>
          </tr>

          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img style="width:105%;max-width:105%" src="images/thriftydagger.png">
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://openreview.net/forum?id=KKBfrCzCVOn">
                <papertitle>ThriftyDAgger: Budget-Aware Novelty and Risk Gating for Interactive Imitation Learning</papertitle>
              </a>
              <br>
              Ryan Hoque, Ashwin Balakrishna, Ellen Novoseller, Daniel S. Brown, <b>Albert Wilcox</b>, Ken Goldberg
              <br>
              <em style="font-style:normal"><i>Conference on Robot Learning (CoRL) 2021.</i> <b>Oral Presentation (6.5% of papers).</b></em>
              <br>
              <a href="https://openreview.net/forum?id=KKBfrCzCVOn">PDF</a> /
              <a href="https://sites.google.com/view/thrifty-dagger/home">Website</a> /
              <a href="data/thriftydagger-bib.txt">Bibtex</a>
              <br>
              <p></p>
              <p>An interactive imitation learning algorithm that reasons about both state novelty and risk to actively query for human interventions. The algorithm balances supervisor burden and task performance more successfully than prior robot-gated algorithms and is competitive with an oracle human-gated baseline.</p>
            </td>
          </tr>

          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img style="width:105%;max-width:105%" src="images/trajectory-dyn.png">
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://arxiv.org/abs/2012.09156">
                <papertitle>Learning Accurate Long-term Dynamics for Model-based Reinforcement Learning</papertitle>
              </a>
              <br>
              Nathan O Lambert, <b>Albert Wilcox</b>, Howard Zhang, Kristofer SJ Pister, Roberto Calandra
              <br>
              <em style="font-style:normal"><i>IEEE Conference on Decision and Control (CDC) 2021.</i></em>
              <br>
              <a href="https://arxiv.org/abs/2012.09156">PDF</a> /
              <a href="https://www.natolambert.com/papers/2020-long-term-dynamics">Website</a> /
              <a href="data/trajectory-bib.txt">Bibtex</a>
              <br>
              <p></p>
              <p>Reframing the model-based RL framework with long-term rather than single step state predictions using continuous "trajectory-based" models.</p>
            </td>
          </tr>



<!--          <tr onmouseout="ibrnet_stop()" onmouseover="ibrnet_start()">-->
<!--            <td style="padding:20px;width:25%;vertical-align:middle">-->
<!--              <div class="one">-->

<!--                <img src='images/ibrnet_before.jpg' width="160">-->
<!--              </div>-->
<!--              <script type="text/javascript">-->
<!--                function ibrnet_start() {-->
<!--                  document.getElementById('ibrnet_image').style.opacity = "1";-->
<!--                }-->

<!--                function ibrnet_stop() {-->
<!--                  document.getElementById('ibrnet_image').style.opacity = "0";-->
<!--                }-->
<!--                ibrnet_stop()-->
<!--              </script>-->
<!--            </td>-->
<!--            <td style="padding:20px;width:75%;vertical-align:middle">-->
<!--              <a href="https://ibrnet.github.io/">-->
<!--                <papertitle>IBRNet: Learning Multi-View Image-Based Rendering</papertitle>-->
<!--              </a>-->
<!--              <br>-->
<!--              <a href="https://www.cs.cornell.edu/~qqw/">Qianqian Wang</a>,-->
<!--              <a href="https://www.linkedin.com/in/zhicheng-wang-96116897/">Zhicheng Wang</a>,-->
<!--              <a href="https://www.kylegenova.com/">Kyle Genova</a>,-->
<!--              <a href="https://people.eecs.berkeley.edu/~pratul/">Pratul Srinivasan</a>,-->
<!--              <a href="https://scholar.google.com/citations?user=Rh9T3EcAAAAJ&hl=en">Howard Zhou</a>, <br>-->
<!--              <strong>Jonathan T. Barron</strong>, -->
<!--              <a href="http://www.ricardomartinbrualla.com/">Ricardo Martin-Brualla</a>,-->
<!--              <a href="https://www.cs.cornell.edu/~snavely/">Noah Snavely</a>, -->
<!--              <a href="https://www.cs.princeton.edu/~funk/">Thomas Funkhouser</a>-->
<!--              <br>-->
<!--              <em>CVPR</em>, 2021-->
<!--              <br>-->
<!--              <a href="https://ibrnet.github.io/">project page</a> /-->
<!--              <a href="https://arxiv.org/abs/2102.13090">arXiv</a>-->
<!--              <p></p>-->
<!--              <p>By learning how to pay attention to input images at render time, -->
<!--                  we can amortize inference for view synthesis and reduce error rates by 15%.</p>-->
<!--            </td>-->
<!--          </tr>-->

<!------------------------------------------------------------------------------------------------->
<!--                              -->
<!------------------------------------------------------------------------------------------------->

        </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
            <td style="padding:0px">
              <br>
              <p style="text-align:right;font-size:small;">
                As with the other 90% of budding ML researchers, I got my website template from <a href="https://jonbarron.info/">Jon Barron.</a>
              </p>
            </td>
          </tr>
        </tbody></table>
      </td>
    </tr>
  </table>
</body>

</html>
