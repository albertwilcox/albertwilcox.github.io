<!DOCTYPE HTML>
<html lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

  <title>Albert Wilcox</title>
  
  <meta name="author" content="Albert Wilcox">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  
  <link rel="stylesheet" type="text/css" href="stylesheet.css">
  <link rel="icon" type="image/png" href="images/seal_icon.png">
</head>

<base target="_blank">

<body>
  <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
    <tr style="padding:0px">
      <td style="padding:0px">
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr style="padding:0px">
            <td style="padding:2.5%;width:63%;vertical-align:middle">
              <p style="text-align:center">
                <name>Albert Wilcox</name>
              </p>
              <p>I am a PhD student in the <a href="https://www.gatech.edu/">Georgia Tech</a> <a href="https://ic.gatech.edu/">
                School of Interactive Computing</a> studying machine learning and robotics.
                <!-- I am advised by <a href="https://goldberg.berkeley.edu/">Professor Ken Goldberg</a> and work in his lab, <a href="http://autolab.berkeley.edu/">UC Berkeley AUTOLab</a>
                which is affiliated with <a href="https://bair.berkeley.edu/">BAIR</a>. -->
              </p>

              <p>Previously I did my BA and MS at <a href="https://bair.berkeley.edu/">BAIR</a> with <a href="https://goldberg.berkeley.edu/">Professor Ken Goldberg</a>.
              </p>

              <p>
                Beyond research, I enjoy outdoor activities of any sort, climbing, playing guitar, and endurance
                sports (I'm on Berkeley's <a href="https://www.caltriathlon.com/">triathlon team</a>).
                Feel free to follow me on <a href="https://www.strava.com/athletes/21827840">Strava</a>!
              </p>
              <p style="text-align:center">
                <a href="mailto:albertwilcox@berkeley.edu">Email</a> &nbsp/&nbsp
                <a href="data/albert-wilcox-cv.pdf">CV</a> &nbsp/&nbsp
                <a href="https://scholar.google.com/citations?user=bj628LsAAAAJ&hl=en">Google Scholar</a> &nbsp/&nbsp
                <a href="https://twitter.com/albertwilcoxiii">Twitter</a> &nbsp/&nbsp
                <a href="https://github.com/albertwilcox">Github</a> &nbsp/&nbsp
                <a href="https://www.linkedin.com/in/albert-wilcox-314898184/">LinkedIn</a>
              </p>
            </td>
            <td style="padding:2.5%;width:40%;max-width:40%">
              <a href="images/albert.jpeg"><img style="width:100%;max-width:100%" alt="profile photo" src="images/albert.jpeg" class="hoverZoomLink"></a>
            </td>
          </tr>
        </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <heading>Research</heading>
              <p>
                I'm broadly interested in learning for decision making and control, and have spent time working with
                reinforcement learning, imitation learning, representation learning and computer vision.
                I'm most excited about studying methods for scaling RL to physical settings, and have 
                studied this problem focusing on applications to robotic automation and self driving.
              </p>
            </td>
          </tr>
        </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>

          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img style="width:105%;max-width:105%" src="images/tacvis.png">
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://arxiv.org/abs/2209.13042">
                <papertitle>Self-Supervised Visuo-Tactile Pretraining to Locate and Follow Garment Features</papertitle>
              </a>
              <br>
              Justin Kerr, Huang Huang, <b>Albert Wilcox</b>, Ryan Hoque, Jeffrey Ichnowski, Roberto Calandra, and Ken Goldberg.
              <br>
              <em style="font-style:normal"><i>Robotics Science and Systems (RSS), 2023.</i></em>
              <br>
              <a href="https://arxiv.org/abs/2209.13042">PDF</a> / 
              <a href="https://sites.google.com/berkeley.edu/ssvtp">Website</a>
              <br>
              <p></p>
              <p>Learning a shared latent space between visual and tactile observations which is useful for a variety of downstream tasks.</p>
            </td>
          </tr>

          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img style="width:105%;max-width:105%" src="images/mcac.png">
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://arxiv.org/abs/2210.07432">
                <papertitle>Monte Carlo Augmented Actor-Critic for Sparse Reward Deep Reinforcement Learning from Suboptimal Demonstrations</papertitle>
              </a>
              <br>
              <b>Albert Wilcox</b>, Ashwin Balakrishna, Daniel Brown, Jules Dedieu, Wyame Benslimane, Ken Goldberg
              <br>
              <em style="font-style:normal"><i>Conference on Neural Information Processing Systems (NeurIPS), 2022.</i></em>
              <br>
              <a href="https://arxiv.org/abs/2210.07432">PDF</a> /
              <a href="https://sites.google.com/view/mcac-rl">Website</a> /
              <a href="data/mcac-bib.txt">Bibtex</a>
              <br>
              <p></p>
              <p>An easy-to-implement change that can be made to any off-policy actor critic algorithm to speed up and stabilize sparse reward deep reinforcement learning from demonstrations.</p>
            </td>
          </tr>

          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img style="width:105%;max-width:105%" src="images/houston.png">
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://sites.google.com/berkeley.edu/autolab-houston">
                <papertitle>Learning to Localize, Grasp and Hand Over Unmodified Surgical Needles</papertitle>
              </a>
              <br>
              <b>Albert Wilcox*</b>, Justin Kerr*, Brijen Thananjeyan, Jeff Ichnowski, Minho Hwang, Samuel Paradis, Danyal Fer, Ken Goldberg
              <br>
              <em style="font-style:normal"><i>IEEE International Conference on Robotics and Automation (ICRA), 2022.</i></em>
              <br>
              <a href="https://arxiv.org/abs/2112.04071">PDF</a> /
              <a href="https://sites.google.com/berkeley.edu/autolab-houston">Website</a> /
              <a href="data/houston-bib.txt">Bibtex</a>
              <br>
              <p></p>
              <p>An algorithm combining active sensing, perception and imitation learning to reliably hand unmodified surgical needles from one arm to the other on a daVinci Research Kit surgical robot.</p>
            </td>
          </tr>

          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img style="width:105%;max-width:105%" src="images/ls3.png">
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://openreview.net/forum?id=kgoWLlA33-U">
                <papertitle>LS3: Latent Space Safe Sets for Long-Horizon Visuomotor Control of Iterative Tasks</papertitle>
              </a>
              <br>
              <b>Albert Wilcox*</b>, Ashwin Balakrishna*, Brijen Thananjeyan, Joseph E. Gonzalez, Ken Goldberg
              <br>
              <em style="font-style:normal"><i>Conference on Robot Learning (CoRL) 2021.</i></em>
              <br>
              <a href="https://arxiv.org/pdf/2107.04775.pdf">PDF</a> /
              <a href="https://sites.google.com/view/latentspacesafesets">Website</a> /
              <a href="data/ls3-bib.txt">Bibtex</a>
              <br>
              <p></p>
              <p>Safe and efficient RL from image observations by leveraging suboptimal demonstrations to structure exploration and examples of constraint violations to satisfy user-specified constraints.</p>
            </td>
          </tr>

          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img style="width:105%;max-width:105%" src="images/thriftydagger.png">
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://openreview.net/forum?id=KKBfrCzCVOn">
                <papertitle>ThriftyDAgger: Budget-Aware Novelty and Risk Gating for Interactive Imitation Learning</papertitle>
              </a>
              <br>
              Ryan Hoque, Ashwin Balakrishna, Ellen Novoseller, Daniel S. Brown, <b>Albert Wilcox</b>, Ken Goldberg
              <br>
              <em style="font-style:normal"><i>Conference on Robot Learning (CoRL) 2021.</i> <b>Oral Presentation (6.5% of papers).</b></em>
              <br>
              <a href="https://openreview.net/forum?id=KKBfrCzCVOn">PDF</a> /
              <a href="https://sites.google.com/view/thrifty-dagger/home">Website</a> /
              <a href="data/thriftydagger-bib.txt">Bibtex</a>
              <br>
              <p></p>
              <p>An interactive imitation learning algorithm that reasons about both state novelty and risk to actively query for human interventions. The algorithm balances supervisor burden and task performance more successfully than prior robot-gated algorithms and is competitive with an oracle human-gated baseline.</p>
            </td>
          </tr>

          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img style="width:105%;max-width:105%" src="images/trajectory-dyn.png">
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://arxiv.org/abs/2012.09156">
                <papertitle>Learning Accurate Long-term Dynamics for Model-based Reinforcement Learning</papertitle>
              </a>
              <br>
              Nathan O Lambert, <b>Albert Wilcox</b>, Howard Zhang, Kristofer SJ Pister, Roberto Calandra
              <br>
              <em style="font-style:normal"><i>IEEE Conference on Decision and Control (CDC) 2021.</i></em>
              <br>
              <a href="https://arxiv.org/abs/2012.09156">PDF</a> /
              <a href="https://www.natolambert.com/papers/2020-long-term-dynamics">Website</a> /
              <a href="data/trajectory-bib.txt">Bibtex</a>
              <br>
              <p></p>
              <p>Reframing the model-based RL framework with long-term rather than single step state predictions using continuous "trajectory-based" models.</p>
            </td>
          </tr>



<!--          <tr onmouseout="ibrnet_stop()" onmouseover="ibrnet_start()">-->
<!--            <td style="padding:20px;width:25%;vertical-align:middle">-->
<!--              <div class="one">-->

<!--                <img src='images/ibrnet_before.jpg' width="160">-->
<!--              </div>-->
<!--              <script type="text/javascript">-->
<!--                function ibrnet_start() {-->
<!--                  document.getElementById('ibrnet_image').style.opacity = "1";-->
<!--                }-->

<!--                function ibrnet_stop() {-->
<!--                  document.getElementById('ibrnet_image').style.opacity = "0";-->
<!--                }-->
<!--                ibrnet_stop()-->
<!--              </script>-->
<!--            </td>-->
<!--            <td style="padding:20px;width:75%;vertical-align:middle">-->
<!--              <a href="https://ibrnet.github.io/">-->
<!--                <papertitle>IBRNet: Learning Multi-View Image-Based Rendering</papertitle>-->
<!--              </a>-->
<!--              <br>-->
<!--              <a href="https://www.cs.cornell.edu/~qqw/">Qianqian Wang</a>,-->
<!--              <a href="https://www.linkedin.com/in/zhicheng-wang-96116897/">Zhicheng Wang</a>,-->
<!--              <a href="https://www.kylegenova.com/">Kyle Genova</a>,-->
<!--              <a href="https://people.eecs.berkeley.edu/~pratul/">Pratul Srinivasan</a>,-->
<!--              <a href="https://scholar.google.com/citations?user=Rh9T3EcAAAAJ&hl=en">Howard Zhou</a>, <br>-->
<!--              <strong>Jonathan T. Barron</strong>, -->
<!--              <a href="http://www.ricardomartinbrualla.com/">Ricardo Martin-Brualla</a>,-->
<!--              <a href="https://www.cs.cornell.edu/~snavely/">Noah Snavely</a>, -->
<!--              <a href="https://www.cs.princeton.edu/~funk/">Thomas Funkhouser</a>-->
<!--              <br>-->
<!--              <em>CVPR</em>, 2021-->
<!--              <br>-->
<!--              <a href="https://ibrnet.github.io/">project page</a> /-->
<!--              <a href="https://arxiv.org/abs/2102.13090">arXiv</a>-->
<!--              <p></p>-->
<!--              <p>By learning how to pay attention to input images at render time, -->
<!--                  we can amortize inference for view synthesis and reduce error rates by 15%.</p>-->
<!--            </td>-->
<!--          </tr>-->

<!------------------------------------------------------------------------------------------------->
<!--                              -->
<!------------------------------------------------------------------------------------------------->

        </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
            <td style="padding:0px">
              <br>
              <p style="text-align:right;font-size:small;">
                As with the other 90% of budding ML researchers, I got my website template from <a href="https://jonbarron.info/">Jon Barron.</a>
              </p>
            </td>
          </tr>
        </tbody></table>
      </td>
    </tr>
  </table>
</body>

</html>
